This is roughly the structure I'm thinking of for this set of projects.
# Level 1: Basic Mini-batch division

## Project 1: Implement basic mini-batching

## Project 2: Mini batch in pytorch

# Level 2: Implementing data sharding across devices

## Project 3: Single-device data sharding

## Project 4: Multi-GPU data sharding with Pytorch

# Level 3: Gradient Aggregation and Synchronous updates

## Project 5: Manual Gradient Aggregation

## Project 6: All-reduce simulation

# Level 4: Optimizing communication overhead

## Project 7: Gradient Compression experiment

## Project 8: Overlapping communication with computation

# Level 5: Distributed data loading and advanced sharding

## Project 9: Custom distributed data loader

## Project 10: Implement consistent shuffling in distributed training